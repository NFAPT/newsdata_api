# NewsData.io & Wikipedia â€“ Pipeline Bronze + SQLite

Exemplo **simples e direto** para ingestÃ£o de dados de duas fontes:

**NewsData.io:**
1. Definir API Key
2. Fazer request na NewsData.io
3. Salvar JSON (raw)
4. Converter JSON â†’ pandas DataFrame
5. Salvar CSV (bronze tabular)
6. Salvar Parquet (bronze tabular)
7. Carregar para base de dados SQLite (opcional)

**Wikipedia (Web Scraping):**
1. Escolher idioma (Portugues ou Ingles)
2. Escolher modo (tema, aleatorio ou URLs manuais)
3. Extrair titulo + resumo de ate 10 paginas
4. Salvar JSON (raw) + CSV (bronze tabular)
5. Carregar para base de dados SQLite (opcional)

## Arquitetura Medallion

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA LAKEHOUSE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ğŸ¥‰ BRONZE     â”‚   ğŸ¥ˆ SILVER     â”‚   ğŸ¥‡ GOLD                   â”‚
â”‚   (Raw Data)    â”‚   (Cleaned)     â”‚   (Business Ready)          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âœ… Este projeto â”‚ â€¢ Dados limpos  â”‚ â€¢ AgregaÃ§Ãµes                â”‚
â”‚ â€¢ JSON da API   â”‚ â€¢ Validados     â”‚ â€¢ KPIs                      â”‚
â”‚ â€¢ CSV tabular   â”‚ â€¢ Tipados       â”‚ â€¢ Prontos para anÃ¡lise      â”‚
â”‚ â€¢ SQLite DB     â”‚                 â”‚                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Estrutura do Projeto

```
newsdata_api/
â”œâ”€â”€ main.py                    â† Ponto de entrada (NewsData.io)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ingest.py          â† IngestÃ£o NewsData.io
â”‚   â”‚   â””â”€â”€ wiki_scraper.py    â† Web Scraping Wikipedia
â”‚   â””â”€â”€ db/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ loader.py           â† Carregamento para SQLite
â”œâ”€â”€ collection/
â”‚   â””â”€â”€ bronze/                â† Dados coletados
â”‚       â”œâ”€â”€ newsdata_{endpoint}_raw_{timestamp}.json
â”‚       â”œâ”€â”€ newsdata_{endpoint}_tabular_{timestamp}.csv
â”‚       â”œâ”€â”€ newsdata_{endpoint}_tabular_{timestamp}.parquet
â”‚       â”œâ”€â”€ wiki_scrape_raw_{timestamp}.json
â”‚       â””â”€â”€ wiki_scrape_tabular_{timestamp}.csv
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ newsdata.db            â† Base de dados SQLite (gerado)
â”‚   â””â”€â”€ wiki.db                â† Base de dados Wikipedia (gerado)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_bronze.py
â”‚   â”œâ”€â”€ test_db.py
â”‚   â””â”€â”€ test_wiki_scraper.py
â”œâ”€â”€ .env                       â† API Key (nÃ£o commitar!)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## InstalaÃ§Ã£o

### Windows (D:\)

```cmd
d:
cd newsdata_api

python -m venv venv
venv\Scripts\activate

pip install -r requirements.txt
```

### Linux/Mac

```bash
cd newsdata_api
python3 -m venv venv
source venv/bin/activate

pip install -r requirements.txt
```

## UtilizaÃ§Ã£o

### Pipeline Bronze (ingestÃ£o)

```bash
python -m src.bronze.ingest
```

No final da execuÃ§Ã£o, Ã© perguntado se queres carregar os dados na base de dados SQLite.

### Endpoints disponÃ­veis

```bash
# NotÃ­cias de Portugal (default)
python -m src.bronze.ingest

# Tech news
python -m src.bronze.ingest --endpoint tech

# Crypto news
python -m src.bronze.ingest --endpoint crypto

# Global market news
python -m src.bronze.ingest --endpoint market
```

### OpÃ§Ãµes adicionais

```bash
# NotÃ­cias de outro paÃ­s
python -m src.bronze.ingest --country br

# NotÃ­cias de uma categoria
python -m src.bronze.ingest --category technology

# Pesquisa por termo
python -m src.bronze.ingest --query "inteligÃªncia artificial"

# Mais resultados
python -m src.bronze.ingest --size 20
```

### Wikipedia (Web Scraping)

```bash
python -m src.bronze.wiki_scraper
```

O script apresenta menus interativos:

1. **Idioma** â€” Portugues (`pt.wikipedia.org`) ou Ingles (`en.wikipedia.org`)
2. **Modo de scraping:**
   - `[1]` Pesquisar por tema â€” introduzir termo de pesquisa
   - `[2]` Paginas aleatorias â€” 10 paginas aleatorias
   - `[3]` URLs manuais â€” introduzir ate 10 URLs da Wikipedia
3. **Carregar na DB** â€” no final, pergunta se quer gravar em `db/wiki.db`

### Carregar para SQLite (standalone)

Carrega todos os CSV de noticias existentes na base de dados:

```bash
python -m src.db.loader
```

Com caminho personalizado:

```bash
python -m src.db.loader --db-path outro_caminho/dados.db
```

## Output

### Bronze Layer (`collection/bronze/`)

| Ficheiro | DescriÃ§Ã£o |
|----------|-----------|
| `newsdata_{endpoint}_raw_{timestamp}.json` | JSON original da API (raw) |
| `newsdata_{endpoint}_tabular_{timestamp}.csv` | Dados tabulares CSV (DataFrame) |
| `newsdata_{endpoint}_tabular_{timestamp}.parquet` | Dados tabulares Parquet (DataFrame) |
| `wiki_scrape_raw_{timestamp}.json` | JSON original Wikipedia (raw) |
| `wiki_scrape_tabular_{timestamp}.csv` | Dados tabulares Wikipedia |

### Base de dados (`db/newsdata.db`)

Tabela `artigos` com as seguintes colunas:

| Coluna | Tipo | DescriÃ§Ã£o |
|--------|------|-----------|
| `article_id` | TEXT (PK) | Identificador Ãºnico do artigo |
| `title` | TEXT | TÃ­tulo |
| `description` | TEXT | Resumo |
| `content` | TEXT | ConteÃºdo (limitado no plano gratuito) |
| `source_id` | TEXT | Identificador da fonte |
| `source_name` | TEXT | Nome da fonte |
| `source_url` | TEXT | URL da fonte |
| `creator` | TEXT | Autor(es) |
| `pubDate` | TEXT | Data de publicaÃ§Ã£o |
| `category` | TEXT | Categoria(s) |
| `country` | TEXT | PaÃ­s |
| `language` | TEXT | Idioma |
| `link` | TEXT | URL do artigo |
| `image_url` | TEXT | URL da imagem |
| `endpoint` | TEXT | Endpoint de origem (latestPT, crypto, etc.) |
| `loaded_at` | TEXT | Timestamp de carregamento |

Duplicados sÃ£o ignorados automaticamente (`INSERT OR IGNORE` por `article_id`).

### Base de dados (`db/wiki.db`)

Tabela `paginas` com as seguintes colunas:

| Coluna | Tipo | DescriÃ§Ã£o |
|--------|------|-----------|
| `pageid` | TEXT (PK) | Identificador da pagina Wikipedia |
| `titulo` | TEXT | Titulo da pagina |
| `resumo` | TEXT | Primeiro paragrafo (extract) |
| `url` | TEXT | URL da pagina |
| `modo` | TEXT | Modo de scraping (tema, aleatorio, urls) |
| `timestamp_scrape` | TEXT | Timestamp da extraÃ§Ã£o |
| `loaded_at` | TEXT | Timestamp de carregamento |

Duplicados sÃ£o ignorados automaticamente (`INSERT OR IGNORE` por `pageid`).

## Pipelines

**NewsData.io:**
```
API â†’ JSON (raw) â†’ DataFrame â†’ CSV + Parquet â†’ SQLite (opcional)
```

**Wikipedia:**
```
Wikipedia API â†’ JSON (raw) â†’ DataFrame â†’ CSV â†’ SQLite (opcional)
```

## Testes

```bash
python -m pytest tests/ -v
```

## Limites da API (Plano Bronze)

- 200 pedidos/dia
- Apenas endpoint `/latest`
- Sem acesso a arquivo histÃ³rico

## LicenÃ§a

MIT
